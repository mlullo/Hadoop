import java.io.*;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;
import org.apache.hadoop.mapred.lib.*;

public class LLM4 extends Configured implements Tool {
	public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, Text> {
		private Text artist = new Text();
		private DoubleWritable duration = new DoubleWritable();

		public void configure(JobConf job) {

		}
       // this is the map function
		public void map(LongWritable key, Text value, OutputCollector<Text, Text> output, Reporter reporter) throws IOException {
		        // this takes a line of input and makes it a string		
			String line = value.toString();
                        // split the inputs on the commas
			String[] split = line.split(",");

			// want to take the years b/tw 2000-2010 (field 165) and collect and output 
			// artist name, duration.
			String artist = split[2].trim();
			String duration = split[3].trim();		 

	    	

		if (Integer.parseInt (split[split.length-223].trim()) >= 2000 && Integer.parseInt(split[split.length-223].trim())<=2010) {
	    		//fields.set(artist+ " "	+duration);
		output.collect(artist, duration);
	    	}
		}
	}	
		
		
	

	public class Partition implements Partitioner <Text,DoubleWritable> {
		@Override
        public int getPartition(Text key, DoubleWritable value, int indexofReducer) {
            char letter = artist.charAt(0);
		//char pivot = "M";
		if(letter>="A"&&letter<="E")
			return 1;
			//put in first partition
		else if(letter>="F"&&letter<="J")
			return 2;
			//put in second partition
		else if(letter>="K"&&letter<="O")
			return 3;
			//put in third partition
		else if (letter>="P"&&letter<="T")
			return 4;
			//put in fourth partition
		else if (letter>="U"&&letter<="Z")
			return 5;
			//put in fifth partition
		}
		@Override
        public void configure(JobConf conf) { 

        }

		

	}

	public static class Reduce extends MapReduceBase implements Reducer<Text, DoubleWritable, Text, DoubleWritable> {
		public void reduce(Text key, Iterator<DoubleWritable> values, OutputCollector<Text, DoubleWritable> output, Reporter reporter) throws IOException {
			double maxDur = 0;

			while(values.hasNext()){
				double value = values.next().get();
				if(value > maxDur)
					maxDur = value;
			}

			output.collect(key, maxDur);
		}
	}


	public int run(String[] args) throws Exception{
		JobConf conf = new JobConf(getConf(), LLM4.class);
		conf.setJobName("LLM4");

		conf.setOutputKeyClass(Text.class);
		conf.setOutputValueClass(DoubleWritable.class);

		conf.setMapperClass(Map.class);
		conf.setNumReduceTasks(5);
		conf.setCombinerClass(Reduce.class);
		conf.setPartitionerClass(Partition.class);
		conf.setReducerClass(Reduce.class);

		conf.setInputFormat(TextInputFormat.class);
		conf.setOutputFormat(TextOutputFormat.class);

		FileInputFormat.setInputPaths(conf, new Path(args[0]));
		FileOutputFormat.setOutputPath(conf, new Path(args[1]));

		JobClient.runJob(conf);
		return 0;
	}	

	public static void main(String[] args) throws Exception{
		int res = ToolRunner.run(new Configuration(), new LLM4(), args);
		System.exit(res);
	}
} 
